{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ccbda8a",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/20357655/145710301-ad00ab66-2378-404f-a918-576aba834ff9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8c8e89",
   "metadata": {},
   "source": [
    "In this homework, we will try to implement a `View Synthesis` model that allows us to generate new scene views based on a single image.\n",
    "\n",
    "The basic idea is to use `differentiable point cloud rendering`, which is used to convert a hidden 3D feature point cloud into a target view.\n",
    "The projected features are decoded by `refinement network` to inpaint missing regions and generate a realistic output image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083c52ab",
   "metadata": {},
   "source": [
    "### Overall pipeline disribed below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90c6e49",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/20357655/145710444-0d0e163f-6996-4eb8-81c0-69798b11c5a6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9712addf",
   "metadata": {},
   "source": [
    "# Data\n",
    "## Download KITTI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d64786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gfile import download_file_from_google_drive\n",
    "\n",
    "\n",
    "# download_file_from_google_drive(\n",
    "#     '1lqspXN10biBShBIVD0yvgnl1nIPPhRdC',\n",
    "#     'kitti.zip'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b12756c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip kitti.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de1f7dd",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7d09277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch3d'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4678b59fb96e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch3d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplotly_vis\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_scene\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch3d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructures\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPointclouds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch3d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPerspectiveCameras\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompositing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrasterize_points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch3d'"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "from tqdm import tqdm\n",
    "from itertools import islice\n",
    "from IPython.display import clear_output, HTML\n",
    "from collections import defaultdict\n",
    "\n",
    "from kitti import KITTIDataLoader\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "\n",
    "from pytorch3d.vis.plotly_vis import plot_scene\n",
    "from pytorch3d.structures import Pointclouds\n",
    "from pytorch3d.renderer import PerspectiveCameras, compositing, rasterize_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6b9b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_RT(RT):\n",
    "    return RT[..., :3, :3], RT[..., :3, 3]\n",
    "\n",
    "def renormalize_image(image):\n",
    "    return image * 0.5 + 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cc1f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = KITTIDataLoader('dataset_kitti')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a536bbe9",
   "metadata": {},
   "source": [
    "Each instance of dataset contain `source` and `target` images, `extrinsic` and `intrinsic` camera parameters for `source` and `targer` images.\n",
    "\n",
    "It is highly recommended to understand these concepts, e.g., here https://ksimek.github.io/2012/08/22/extrinsic/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd27eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, cameras = dataset[0].values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47657b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "ax.imshow(images[0].permute(1, 2, 0) * 0.5 + 0.5)\n",
    "ax.set_title('Source Image Frame', fontsize=20)\n",
    "ax.axis('off')\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "ax.imshow(images[1].permute(1, 2, 0) * 0.5 + 0.5)\n",
    "ax.set_title('Target Image Frame', fontsize=20)\n",
    "ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fdc36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_camera = PerspectiveCameras(\n",
    "    R=split_RT(cameras[0]['P'])[0][None],\n",
    "    T=split_RT(cameras[0]['P'])[1][None],\n",
    "    K=torch.from_numpy(cameras[0]['K'])[None]\n",
    ")\n",
    "\n",
    "target_camera = PerspectiveCameras(\n",
    "    R=split_RT(cameras[1]['P'])[0][None],\n",
    "    T=split_RT(cameras[1]['P'])[1][None],\n",
    "    K=torch.from_numpy(cameras[1]['K'])[None]\n",
    ")\n",
    "\n",
    "plot_scene(\n",
    "    {\n",
    "        'scene': {\n",
    "            'source_camera': source_camera,\n",
    "            'target_camera': target_camera\n",
    "        }\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb87ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = torch.randperm(len(dataset))\n",
    "\n",
    "train_indexes = indexes[:-1000]\n",
    "validation_indexes = indexes[-1000:]\n",
    "\n",
    "train_dataset = Subset(dataset, train_indexes)\n",
    "validation_dataset = Subset(dataset, validation_indexes)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=16, num_workers=6,\n",
    "    shuffle=True, drop_last=True, pin_memory=True\n",
    ")\n",
    "validation_dataloder = DataLoader(\n",
    "    validation_dataset, batch_size=10, num_workers=4,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1463727",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b2df13",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "So, we need to implement `Spatial Feature Predictor`, `Depth Regressor`, `Point Cloud Renderer` and `RefinementNetwork`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e26dacb",
   "metadata": {},
   "source": [
    "One of the main building blocks in these networks is `ResNetBlock`, but with some modifications:\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/20357655/145711537-eebb0cb9-8935-4d65-bc4b-559c1e19ba98.png)\n",
    "\n",
    "So, let's implement it, but without the noise part `Linear + z` (let's omit it, since we do not use the adversarial criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6aa09af",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class ResNetBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        stride: int = 1,\n",
    "        mode = 'identity'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44866d81",
   "metadata": {},
   "source": [
    "## Spatial Feature Predictor\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/20357655/145711931-be08e4f9-f383-4942-8b93-f8bdfd3060d2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76a42f6",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class SpatialFeatureNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels=3, out_channels=64):\n",
    "        super().__init__()\n",
    "        self.blocks = # TODO\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        return self.blocks(input)\n",
    "\n",
    "sf_net = SpatialFeatureNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622d6c5c",
   "metadata": {},
   "source": [
    "## Depth Regressor\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/20357655/145711785-690008e5-96d0-418f-adf1-1509e399c92e.png)\n",
    "\n",
    "An `Enc Block` consists of a sequence of Leaky ReLU, convolution (stride 2, padding 1, kernel size 4), and batch normalisation layers.\n",
    "\n",
    "A `Dec Block` consists of a sequence of ReLU, 2x bilinear upsampling, convolution (stride 1, padding 1, kernel size3), and batch normalisation layers (except for the final layer, which has no batch normalisation layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd12e4c",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_filters=32,\n",
    "        channels_in=3,\n",
    "        channels_out=3\n",
    "    ):\n",
    "        super(Unet, self).__init__()\n",
    "        # TODO\n",
    "\n",
    "    def forward(self, input):\n",
    "        # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7025ae",
   "metadata": {},
   "source": [
    "## Refinement Network\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/20357655/145711921-45ebf1e5-e852-4c47-8b93-d545f67dc6bf.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f40b316",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class RefinementNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels=64, out_channels=3):\n",
    "        super().__init__()\n",
    "        self.blocks = # TODO\n",
    "    \n",
    "    def forward(self, input: torch.Tensor):\n",
    "        return self.blocks(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4061ce38",
   "metadata": {},
   "source": [
    "## Auxiliary network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc2b81e",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class VGG19(nn.Module):\n",
    "    def __init__(self, requires_grad=False):\n",
    "        super().__init__()\n",
    "        vgg_pretrained_features = torchvision.models.vgg19(\n",
    "            pretrained=True\n",
    "        ).features\n",
    "        self.slice1 = torch.nn.Sequential()\n",
    "        self.slice2 = torch.nn.Sequential()\n",
    "        self.slice3 = torch.nn.Sequential()\n",
    "        self.slice4 = torch.nn.Sequential()\n",
    "        self.slice5 = torch.nn.Sequential()\n",
    "        for x in range(2):\n",
    "            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(2, 7):\n",
    "            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(7, 12):\n",
    "            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(12, 21):\n",
    "            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(21, 30):\n",
    "            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n",
    "        if not requires_grad:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Normalize the image so that it is in the appropriate range\n",
    "        h_relu1 = self.slice1(X)\n",
    "        h_relu2 = self.slice2(h_relu1)\n",
    "        h_relu3 = self.slice3(h_relu2)\n",
    "        h_relu4 = self.slice4(h_relu3)\n",
    "        h_relu5 = self.slice5(h_relu4)\n",
    "        out = [h_relu1, h_relu2, h_relu3, h_relu4, h_relu5]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecc0036",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb955b50",
   "metadata": {},
   "source": [
    "# Criterions & Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4a8159",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class PerceptualLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Set to false so that this part of the network is frozen\n",
    "        self.model = VGG19(requires_grad=False)\n",
    "        self.criterion = nn.L1Loss()\n",
    "        self.weights = [1.0 / 32, 1.0 / 16, 1.0 / 8, 1.0 / 4, 1.0]\n",
    "\n",
    "    def forward(self, pred_img, gt_img):\n",
    "        gt_fs = self.model(gt_img)\n",
    "        pred_fs = self.model(pred_img)\n",
    "\n",
    "        # Collect the losses at multiple layers (need unsqueeze in\n",
    "        # order to concatenate these together)\n",
    "        loss = 0\n",
    "        for i in range(0, len(gt_fs)):\n",
    "            loss += self.weights[i] * self.criterion(pred_fs[i], gt_fs[i])\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81835854",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def psnr(predicted_image, target_image):\n",
    "    batch_size = predicted_image.size(0)\n",
    "    mse_err = (\n",
    "        (predicted_image - target_image)\n",
    "        .pow(2).sum(dim=1)\n",
    "        .view(batch_size, -1).mean(dim=1)\n",
    "    )\n",
    "\n",
    "    psnr = 10 * (1 / mse_err).log10()\n",
    "    return psnr.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e7d6f0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553d5034",
   "metadata": {},
   "source": [
    "# Point Cloud Renderer\n",
    "\n",
    "`Differential Rasterization` is a key component of our system. We will use the algorithm already implemented in `pytorch3d`.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/20357655/145715968-94abbe1a-8d14-4c20-98c4-61afd9161ada.png)\n",
    "\n",
    "For more details read (3.2) https://arxiv.org/pdf/1912.08804.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f59d89",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class PointsRasterizerWithBlending(nn.Module):\n",
    "    \"\"\"\n",
    "    Rasterizes a set of points using a differentiable renderer. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, radius=1.5, image_size=256, points_per_pixel=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.radius = radius\n",
    "        self.image_size = image_size\n",
    "        self.points_per_pixel = points_per_pixel\n",
    "        \n",
    "        self.rad_pow = 2\n",
    "        self.tau = 1.0\n",
    "\n",
    "    def forward(self, point_cloud, spatial_features):\n",
    "        batch_size = spatial_features.size(0)\n",
    "\n",
    "        # Make sure these have been arranged in the same way\n",
    "        assert point_cloud.size(2) == 3\n",
    "        assert point_cloud.size(1) == spatial_features.size(2)\n",
    "\n",
    "        point_cloud[:, :, 1] = -point_cloud[:, :, 1]\n",
    "        point_cloud[:, :, 0] = -point_cloud[:, :, 0]\n",
    "\n",
    "        radius = float(self.radius) / float(self.image_size) * 2.0\n",
    "\n",
    "        point_cloud = Pointclouds(points=point_cloud, features=spatial_features.permute(0, 2, 1))\n",
    "        points_idx, _, dist = rasterize_points(\n",
    "            point_cloud, self.image_size, radius, self.points_per_pixel\n",
    "        )\n",
    "\n",
    "        dist = dist / pow(radius, self.rad_pow)\n",
    "\n",
    "        alphas = (\n",
    "            (1 - dist.clamp(max=1, min=1e-3).pow(0.5))\n",
    "            .pow(self.tau)\n",
    "            .permute(0, 3, 1, 2)\n",
    "        )\n",
    "    \n",
    "        transformed_src_alphas = compositing.alpha_composite(\n",
    "            points_idx.permute(0, 3, 1, 2).long(),\n",
    "            alphas,\n",
    "            point_cloud.features_packed().permute(1, 0),\n",
    "        )\n",
    "\n",
    "        return transformed_src_alphas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8303a60",
   "metadata": {},
   "source": [
    "And `PointsManipulator` do the following steps:\n",
    "\n",
    "    1) Create virtual image place in [normalized coordinate](https://pytorch3d.org/docs/cameras)\n",
    "    2) Move camera according to `regressed depth`\n",
    "    3) Rotate points according to target camera paramers\n",
    "    4) And finally render them with help of `PointsRasterizerWithBlending`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10446563",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class PointsManipulator(nn.Module):\n",
    "    EPS = 1e-5\n",
    "\n",
    "    def __init__(self, image_size):\n",
    "        super().__init__()\n",
    "        # Assume that image plane is square\n",
    "\n",
    "        self.splatter = PointsRasterizerWithBlending(\n",
    "            radius=1.0,\n",
    "            image_size=image_size,\n",
    "            points_per_pixel=128,\n",
    "        )\n",
    "\n",
    "        xs = torch.linspace(0, image_size - 1, image_size) / \\\n",
    "            float(image_size - 1) * 2 - 1\n",
    "        ys = torch.linspace(0, image_size - 1, image_size) / \\\n",
    "            float(image_size - 1) * 2 - 1\n",
    "\n",
    "        xs = xs.view(1, 1, 1, image_size).repeat(1, 1, image_size, 1)\n",
    "        ys = ys.view(1, 1, image_size, 1).repeat(1, 1, 1, image_size)\n",
    "\n",
    "        xyzs = torch.cat(\n",
    "            (xs, -ys, -torch.ones(xs.size()), torch.ones(xs.size())), 1\n",
    "        ).view(1, 4, -1)\n",
    "\n",
    "        self.register_buffer(\"xyzs\", xyzs)\n",
    "\n",
    "    def project_pts(self, depth, K, K_inv, RT_cam1, RTinv_cam1, RT_cam2, RTinv_cam2):\n",
    "        # Project the world points into the new view\n",
    "        projected_coors = self.xyzs * depth\n",
    "        projected_coors[:, -1, :] = 1\n",
    "\n",
    "        # Transform into camera coordinate of the first view\n",
    "        cam1_X = K_inv.bmm(projected_coors)\n",
    "\n",
    "        # Transform into world coordinates\n",
    "        RT = RT_cam2.bmm(RTinv_cam1)\n",
    "\n",
    "        wrld_X = RT.bmm(cam1_X)\n",
    "\n",
    "        # And intrinsics\n",
    "        xy_proj = K.bmm(wrld_X)\n",
    "\n",
    "        # And finally we project to get the final result\n",
    "        mask = (xy_proj[:, 2:3, :].abs() < self.EPS).detach()\n",
    "\n",
    "        # Remove invalid zs that cause nans\n",
    "        zs = xy_proj[:, 2:3, :]\n",
    "        zs[mask] = self.EPS\n",
    "\n",
    "        sampler = torch.cat((xy_proj[:, 0:2, :] / -zs, xy_proj[:, 2:3, :]), 1)\n",
    "        sampler[mask.repeat(1, 3, 1)] = -10\n",
    "        # Flip the ys\n",
    "        sampler = sampler * torch.Tensor([1, -1, -1]).unsqueeze(0).unsqueeze(\n",
    "            2\n",
    "        ).to(sampler.device)\n",
    "\n",
    "        return sampler\n",
    "\n",
    "    def forward_justpts(\n",
    "        self,\n",
    "        spatial_features, depth,\n",
    "        K, K_inv, RT_cam1, RTinv_cam1, RT_cam2, RTinv_cam2\n",
    "    ):\n",
    "        # Now project these points into a new view\n",
    "        batch_size, c, w, h = spatial_features.size()\n",
    "\n",
    "        if len(depth.size()) > 3:\n",
    "            # reshape into the right positioning\n",
    "            depth = depth.view(batch_size, 1, -1)\n",
    "            spatial_features = spatial_features.view(batch_size, c, -1)\n",
    "\n",
    "        pointcloud = self.project_pts(\n",
    "            depth, K, K_inv, RT_cam1, RTinv_cam1, RT_cam2, RTinv_cam2\n",
    "        )\n",
    "        pointcloud = pointcloud.permute(0, 2, 1).contiguous()\n",
    "        result = self.splatter(pointcloud, spatial_features)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0641cd48",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5ab269",
   "metadata": {},
   "source": [
    "# All together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8be184",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViewSynthesisModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.spatial_feature_predictor = SpatialFeatureNetwork()\n",
    "        self.depth_regressor = Unet(channels_in=3, channels_out=1)\n",
    "        self.point_cloud_renderer = PointsManipulator(image_size=256)\n",
    "        self.refinement_network = RefinementNetwork()\n",
    "\n",
    "        # Special constant for KITTI dataset\n",
    "        self.z_min = 1.0\n",
    "        self.z_max = 50.0\n",
    "\n",
    "    def forward(self):\n",
    "        # TODO\n",
    "        # 1) Predict spatial feature for source image\n",
    "        # 2) Predict depth for source image (dont forget to renormalize depth with z_min/z_max)\n",
    "        # 3) Generate new features with `point_cloud_renderer`\n",
    "        # 4) And finnaly apply `refinement_network` to obtain new image\n",
    "        # 5) return new image, and depth of source image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3978e549",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b553c72",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "In order for the work to be accepted, you must achieve a quality of ~0.5 (validation loss value) and visualize several samples as in the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7f94ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "\n",
    "model = ViewSynthesisModel().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-4)\n",
    "histoty = defaultdict(list)\n",
    "\n",
    "l1_criterion = nn.L1Loss()\n",
    "perceptual_criterion = PerceptualLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2486550",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    for i, batch in tqdm(enumerate(train_dataloader, 2), total=len(train_dataloader)):\n",
    "        source_image = batch[\"images\"][0].to(device)\n",
    "        target_image = batch[\"images\"][-1].to(device)\n",
    "        \n",
    "        # TODO\n",
    "        generated_image, regressed_depth = model(...)\n",
    "\n",
    "        loss = l1_criterion(generated_image, target_image) \\\n",
    "            + 10 * perceptual_criterion(\n",
    "            renormalize_image(generated_image),\n",
    "            renormalize_image(target_image)\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        histoty['train_loss'].append(loss.item())\n",
    "    \n",
    "    for i, batch in tqdm(enumerate(validation_dataloder), total=len(validation_dataloder)):\n",
    "        source_image = batch[\"images\"][0].to(device)\n",
    "        target_image = batch[\"images\"][-1].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # TODO\n",
    "            generated_image, regressed_depth = model(...)\n",
    "\n",
    "        loss = l1_criterion(generated_image, target_image) \\\n",
    "            + 10 * perceptual_criterion(\n",
    "            renormalize_image(generated_image),\n",
    "            renormalize_image(target_image)\n",
    "        )\n",
    "\n",
    "        histoty['validation_loss'].append(loss.item())\n",
    "    \n",
    "        \n",
    "    clear_output()\n",
    "    fig = plt.figure(figsize=(30, 15), dpi=80)\n",
    "    ax1 = plt.subplot(3, 3, 1)\n",
    "    ax1.plot(histoty['train_loss'], label='Train')\n",
    "    ax1.set_xlabel('Iterations', fontsize=20)\n",
    "    ax1.set_ylabel(r'${L_1} + Perceptual$', fontsize=20)\n",
    "    ax1.legend()\n",
    "    ax1.grid()\n",
    "    \n",
    "    ax2 = plt.subplot(3, 3, 4)\n",
    "    ax2.plot(histoty['validation_loss'], label='Validation')\n",
    "    ax2.set_xlabel('Iterations', fontsize=20)\n",
    "    ax2.set_ylabel(r'${L_1} + Perceptual$', fontsize=20)\n",
    "    ax2.legend()\n",
    "    ax2.grid()\n",
    "\n",
    "    for index, image in zip(\n",
    "        (2, 3, 5, 6),\n",
    "        (source_image, target_image, generated_image, regressed_depth)\n",
    "    ):\n",
    "        ax = plt.subplot(3, 3, index)\n",
    "        im = ax.imshow(renormalize_image(image.detach().cpu()[0]).permute(1, 2, 0))\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8559f1ea",
   "metadata": {},
   "source": [
    "# Visualize\n",
    "\n",
    "Goes along depth and generate new views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c941802a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RTs = []\n",
    "for i in torch.linspace(0, 0.5, 40):\n",
    "    current_RT = torch.eye(4).unsqueeze(0)\n",
    "    current_RT[:, 2, 3] = i\n",
    "    RTs.append(current_RT.to(device))\n",
    "identity_matrx = torch.eye(4).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d775b314",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_instance_index = 245\n",
    "\n",
    "with torch.no_grad():\n",
    "    images, cameras = validation_dataset[random_instance_index].values()\n",
    "    # Input values\n",
    "    input_img = images[0][None].cuda()\n",
    "\n",
    "    # Camera parameters\n",
    "    K = torch.from_numpy(cameras[0][\"K\"])[None].to(device)\n",
    "    K_inv = torch.from_numpy(cameras[0][\"Kinv\"])[None].to(device)\n",
    "    \n",
    "    spatial_features = model.spatial_feature_predictor(input_img)\n",
    "    regressed_depth = torch.sigmoid(model.depth_regressor(input_img)) * \\\n",
    "        (model.z_max - model.z_min) + model.z_min\n",
    "\n",
    "    new_images = []\n",
    "    for current_RT in RTs:\n",
    "        generated_features = model.point_cloud_renderer.forward_justpts(\n",
    "            spatial_features,\n",
    "            regressed_depth,\n",
    "            K,\n",
    "            K_inv,\n",
    "            identity_matrx,\n",
    "            identity_matrx,\n",
    "            current_RT,\n",
    "            None\n",
    "        )\n",
    "        generated_image = model.refinement_network(generated_features)\n",
    "        new_images.append(renormalize_image(generated_image.cpu()).clamp(0, 1).mul(255).to(torch.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563bb460",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = torch.cat(new_images).permute(0, 2, 3, 1)\n",
    "torchvision.io.write_video('video.mp4', frames, fps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602c87f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "    <video width=\"256\" alt=\"test\" controls>\n",
    "        <source src=\"video.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822381d1",
   "metadata": {},
   "source": [
    "# Quality benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7aa5f3",
   "metadata": {},
   "source": [
    "![gif](benchmark_video_v1.gif)\n",
    "![gif](benchmark_video_v2.gif)\n",
    "![gif](benchmark_video_v3.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc06bee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
