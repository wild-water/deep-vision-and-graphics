{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3eb8d63",
   "metadata": {},
   "source": [
    "![image](https://raw.githubusercontent.com/facebookresearch/pytorch3d/main/.github/pytorch3dlogo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417e9383",
   "metadata": {},
   "source": [
    "    PyTorch3D provides efficient, reusable components for 3D Computer Vision research with PyTorch.\n",
    "\n",
    "    Key features include:\n",
    "        - Data structure for storing and manipulating triangle meshes, pointclouds, virtual cameras, volumes and so on\n",
    "        - Efficient operations on triangle meshes (projective transformations, graph convolution, sampling, loss functions)\n",
    "        - A differentiable mesh and pointcloud renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement pytorch3d==0.6.1 (from versions: 0.0.1, 0.1.1, 0.2.0, 0.2.5, 0.3.0)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for pytorch3d==0.6.1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install pytorch3d==0.6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22437521",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import pytorch3d\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b644c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert pytorch3d.__version__.startswith('0.6')\n",
    "assert torch.__version__.startswith('1.9.1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c163cacb",
   "metadata": {},
   "source": [
    "# 3D Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fd330b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch3d.io import load_objs_as_meshes\n",
    "from pytorch3d.structures import Pointclouds, Meshes\n",
    "from pytorch3d.vis.plotly_vis import plot_scene\n",
    "from pytorch3d.vis.texture_vis import texturesuv_image_matplotlib\n",
    "from pytorch3d.renderer import (\n",
    "    look_at_view_transform,\n",
    "    FoVOrthographicCameras,\n",
    "    FoVPerspectiveCameras,\n",
    "    PointsRasterizer,\n",
    "    PointsRasterizationSettings,\n",
    "    PointsRenderer,\n",
    "    AlphaCompositor,\n",
    "    RasterizationSettings,\n",
    "    PointLights,\n",
    "    MeshRenderer,\n",
    "    MeshRasterizer,\n",
    "    SoftPhongShader\n",
    ")\n",
    "\n",
    "DEVICE = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edc29ce",
   "metadata": {},
   "source": [
    "## Pointclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d24867",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data/PittsburghBridge\n",
    "!wget -P data/PittsburghBridge https://dl.fbaipublicfiles.com/pytorch3d/data/PittsburghBridge/pointcloud.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4973e2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_filename = Path('data/PittsburghBridge/pointcloud.npz')\n",
    "\n",
    "raw_pointcloud = np.load(obj_filename)\n",
    "\n",
    "vertexes = torch.from_numpy(raw_pointcloud['verts'])\n",
    "rgb = torch.from_numpy(raw_pointcloud['rgb'])\n",
    "pointcloud = Pointclouds(points=[vertexes], features=[rgb]).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141307c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scene({\n",
    "    'scene': {\n",
    "        'pointcloud': pointcloud\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e44956e",
   "metadata": {},
   "source": [
    "## Render Pointclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443936c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "R, T = look_at_view_transform(20, 10, 0)\n",
    "cameras = FoVOrthographicCameras(device=DEVICE, R=R, T=T, znear=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a854ffe",
   "metadata": {},
   "source": [
    "### Feel free to rotate the camera position in plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d3ba88",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scene({\n",
    "    'scene': {\n",
    "        'pointcloud': pointcloud,\n",
    "        'camera': cameras\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d867df27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the settings for rasterization and shading. Here we set the output image to be of size\n",
    "# 512x512. As we are rendering images for visualization purposes only we will set faces_per_pixel=1\n",
    "# and blur_radius=0.0. Refer to raster_points.py for explanations of these parameters.\n",
    "raster_settings = PointsRasterizationSettings(\n",
    "    image_size=512,\n",
    "    radius=0.003,\n",
    "    points_per_pixel=10\n",
    ")\n",
    "\n",
    "\n",
    "# Create a points renderer by compositing points using an alpha compositor (nearer points\n",
    "# are weighted more heavily).\n",
    "rasterizer = PointsRasterizer(cameras=cameras, raster_settings=raster_settings)\n",
    "renderer = PointsRenderer(\n",
    "    rasterizer=rasterizer,\n",
    "    compositor=AlphaCompositor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda044da",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = renderer(pointcloud)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(images[0, ..., :3].cpu().numpy())\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9bd770",
   "metadata": {},
   "source": [
    "## Meshes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb71e827",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -P data/cow_mesh https://dl.fbaipublicfiles.com/pytorch3d/data/cow_mesh/cow.obj\n",
    "!wget -P data/cow_mesh https://dl.fbaipublicfiles.com/pytorch3d/data/cow_mesh/cow.mtl\n",
    "!wget -P data/cow_mesh https://dl.fbaipublicfiles.com/pytorch3d/data/cow_mesh/cow_texture.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e677fcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_filename = Path('data/cow_mesh/cow.obj')\n",
    "mesh = load_objs_as_meshes([obj_filename], device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc22edc",
   "metadata": {},
   "source": [
    "### Only mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be367eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scene({\n",
    "    'scene': {\n",
    "        'mesh': mesh,\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37f07cf",
   "metadata": {},
   "source": [
    "### Texture map and UV points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e73362e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "texturesuv_image_matplotlib(mesh.textures, subsample=None)\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee1e5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a camera.\n",
    "# With world coordinates +Y up, +X left and +Z in, the front of the cow is facing the -Z direction.\n",
    "# So we move the camera by 180 in the azimuth direction so it is facing the front of the cow.\n",
    "R, T = look_at_view_transform(2.7, 0, 180)\n",
    "cameras = FoVPerspectiveCameras(device=DEVICE, R=R, T=T)\n",
    "\n",
    "# Define the settings for rasterization and shading. Here we set the output image to be of size\n",
    "# 512x512. As we are rendering images for visualization purposes only we will set faces_per_pixel=1\n",
    "# and blur_radius=0.0. We also set bin_size and max_faces_per_bin to None which ensure that\n",
    "# the faster coarse-to-fine rasterization method is used. Refer to rasterize_meshes.py for\n",
    "# explanations of these parameters. Refer to docs/notes/renderer.md for an explanation of\n",
    "# the difference between naive and coarse-to-fine rasterization.\n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size=512,\n",
    "    blur_radius=0.0,\n",
    "    faces_per_pixel=1,\n",
    ")\n",
    "\n",
    "# Place a point light in front of the object. As mentioned above, the front of the cow is facing the\n",
    "# -z direction.\n",
    "lights = PointLights(device=DEVICE, location=[[0.0, 0.0, -3.0]])\n",
    "\n",
    "# Create a Phong renderer by composing a rasterizer and a shader. The textured Phong shader will\n",
    "# interpolate the texture uv coordinates for each vertex, sample from a texture image and\n",
    "# apply the Phong lighting model\n",
    "renderer = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(\n",
    "        cameras=cameras,\n",
    "        raster_settings=raster_settings\n",
    "    ),\n",
    "    shader=SoftPhongShader(\n",
    "        device=DEVICE,\n",
    "        cameras=cameras,\n",
    "        lights=lights\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cd0651",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = renderer(mesh)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(images[0, ..., :3].cpu().numpy())\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686d2b8e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8a3fbf",
   "metadata": {},
   "source": [
    "# Cameras and Rays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ffe71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch3d.renderer import FoVOrthographicCameras, RayBundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477cc32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_camera = FoVPerspectiveCameras()\n",
    "\n",
    "# Rays that are used, for example, in NeRF\n",
    "origin_rays = RayBundle(\n",
    "    origins=torch.zeros(100, 3),\n",
    "    directions=torch.nn.functional.normalize(torch.rand(100, 3), dim=-1),\n",
    "    lengths=torch.ones(100, 3),\n",
    "    xys=torch.tensor([1]).view(1, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded85062",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scene({\n",
    "    'scene': {\n",
    "        'base_camera': origin_camera,\n",
    "        'ray': origin_rays\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e7edf7",
   "metadata": {},
   "source": [
    "## Translate and Rotate Camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72810774",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch3d.transforms import Translate, Rotate, random_rotation, Transform3d\n",
    "from pytorch3d.renderer.cameras import CamerasBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90990d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_transform = (\n",
    "    Transform3d()\n",
    "    .translate(1, 1, 1)\n",
    "    .rotate(random_rotation())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e21c96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_camera_matrix = Transform3d().rotate(origin_camera.R).translate(origin_camera.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cf0553",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_camera_matrix = camera_transform.compose(origin_camera_matrix)\n",
    "result_R = result_camera_matrix.get_matrix()[..., :3, :3]\n",
    "result_T = result_camera_matrix.get_matrix()[..., 3, :3]\n",
    "\n",
    "new_camera = FoVPerspectiveCameras(R=result_R, T=result_T, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52d816d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scene({\n",
    "    'scene': {\n",
    "        'base_camera': origin_camera,\n",
    "        'transformed_camera': new_camera\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8978f361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1122e4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
